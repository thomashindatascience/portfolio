{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "nlp-sequence-models",
      "graded_item_id": "8hb5s",
      "launcher_item_id": "5NrJ6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Solving word analogy with word vector.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GRMlnr6i9PD",
        "colab_type": "text"
      },
      "source": [
        "# Operations on word vectors\n",
        "\n",
        "\n",
        "- Load pre-trained word vectors, and measure similarity using cosine similarity\n",
        "- Use word embeddings to solve word analogy problems such as Man is to Woman as King is to ______. \n",
        "- Modify word embeddings to reduce their gender bias \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wziuScTai9PH",
        "colab_type": "code",
        "outputId": "ed0b9515-3815-406d-d6a5-b78c5cba443f",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from w2v_utils import *"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvoCbOs6i9Pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words, word_to_vec_map = read_glove_vecs('../../readonly/glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJLnpUKzi9QA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Cosine similarity reflects the degree of similariy between u and v\n",
        "        \n",
        "    Arguments:\n",
        "        u -- a word vector of shape (n,)          \n",
        "        v -- a word vector of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
        "    \"\"\"\n",
        "    \n",
        "    distance = 0.0\n",
        "    dot = np.dot(u,v)\n",
        "    norm_u = np.sqrt(np.sum(np.square(u)))\n",
        "    norm_v = np.sqrt(np.sum(np.square(v)))\n",
        "    cosine_similarity = dot/(norm_u*norm_v)\n",
        "    \n",
        "    return cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPIzpPDIi9QL",
        "colab_type": "code",
        "outputId": "9c57bcbf-e24f-40d4-e92e-ae9428849e2f",
        "colab": {}
      },
      "source": [
        "father = word_to_vec_map[\"father\"]\n",
        "mother = word_to_vec_map[\"mother\"]\n",
        "ball = word_to_vec_map[\"ball\"]\n",
        "crocodile = word_to_vec_map[\"crocodile\"]\n",
        "france = word_to_vec_map[\"france\"]\n",
        "italy = word_to_vec_map[\"italy\"]\n",
        "paris = word_to_vec_map[\"paris\"]\n",
        "rome = word_to_vec_map[\"rome\"]\n",
        "\n",
        "print(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\n",
        "print(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\n",
        "print(\"cosine_similarity(france - paris, rome - italy) = \",cosine_similarity(france - paris, rome - italy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cosine_similarity(father, mother) =  0.890903844289\n",
            "cosine_similarity(ball, crocodile) =  0.274392462614\n",
            "cosine_similarity(france - paris, rome - italy) =  -0.675147930817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftPg_nPEi9Qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
        "    \n",
        "    Arguments:\n",
        "    word_a -- a word, string\n",
        "    word_b -- a word, string\n",
        "    word_c -- a word, string\n",
        "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
        "    \n",
        "    Returns:\n",
        "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
        "    \"\"\"\n",
        "\n",
        "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
        "    e_a, e_b, e_c = word_to_vec_map[word_a],word_to_vec_map[word_b],word_to_vec_map[word_c]\n",
        "    \n",
        "    words = word_to_vec_map.keys()\n",
        "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
        "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
        "\n",
        "    for w in words:        \n",
        "        if w in [word_a, word_b, word_c] :\n",
        "            continue\n",
        "\n",
        "        cosine_sim = cosine_similarity(e_b-e_a, word_to_vec_map[w]-e_c)\n",
        "\n",
        "        if cosine_sim > max_cosine_sim:\n",
        "            max_cosine_sim = cosine_sim\n",
        "            best_word = w\n",
        "\n",
        "        \n",
        "    return best_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTtjv_NRi9Q6",
        "colab_type": "code",
        "outputId": "0cb16374-1e6d-41a6-b40b-3e80987374cf",
        "colab": {}
      },
      "source": [
        "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
        "for triad in triads_to_try:\n",
        "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "italy -> italian :: spain -> spanish\n",
            "india -> delhi :: japan -> tokyo\n",
            "man -> woman :: boy -> girl\n",
            "small -> smaller :: large -> larger\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0HsL7hHi9RW",
        "colab_type": "code",
        "outputId": "38056828-bfbd-499b-fce3-4125c41d2927",
        "colab": {}
      },
      "source": [
        "g = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
        "print(g)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.087144    0.2182     -0.40986    -0.03922    -0.1032      0.94165\n",
            " -0.06042     0.32988     0.46144    -0.35962     0.31102    -0.86824\n",
            "  0.96006     0.01073     0.24337     0.08193    -1.02722    -0.21122\n",
            "  0.695044   -0.00222     0.29106     0.5053     -0.099454    0.40445\n",
            "  0.30181     0.1355     -0.0606     -0.07131    -0.19245    -0.06115\n",
            " -0.3204      0.07165    -0.13337    -0.25068714 -0.14293    -0.224957\n",
            " -0.149       0.048882    0.12191    -0.27362    -0.165476   -0.20426\n",
            "  0.54376    -0.271425   -0.10245    -0.32108     0.2516     -0.33455\n",
            " -0.04371     0.01258   ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3o46woeui9Rl",
        "colab_type": "code",
        "outputId": "af3c4cfd-5a5a-4a11-e912-459430a6c31a",
        "colab": {}
      },
      "source": [
        "print ('List of names and their similarities with constructed vector:')\n",
        "\n",
        "# girls and boys name\n",
        "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
        "\n",
        "for w in name_list:\n",
        "    print (w, cosine_similarity(word_to_vec_map[w], g))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List of names and their similarities with constructed vector:\n",
            "john -0.23163356146\n",
            "marie 0.315597935396\n",
            "sophie 0.318687898594\n",
            "ronaldo -0.312447968503\n",
            "priya 0.17632041839\n",
            "rahul -0.169154710392\n",
            "danielle 0.243932992163\n",
            "reza -0.079304296722\n",
            "katy 0.283106865957\n",
            "yasmin 0.233138577679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnXBxKzUi9R0",
        "colab_type": "code",
        "outputId": "1bc1d50a-a70a-4066-a2b5-ccbd829f5dd1",
        "colab": {}
      },
      "source": [
        "print('Other words and their similarities:')\n",
        "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
        "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
        "for w in word_list:\n",
        "    print (w, cosine_similarity(word_to_vec_map[w], g))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Other words and their similarities:\n",
            "lipstick 0.276919162564\n",
            "guns -0.18884855679\n",
            "science -0.0608290654093\n",
            "arts 0.00818931238588\n",
            "literature 0.0647250443346\n",
            "warrior -0.209201646411\n",
            "doctor 0.118952894109\n",
            "tree -0.0708939917548\n",
            "receptionist 0.330779417506\n",
            "technology -0.131937324476\n",
            "fashion 0.0356389462577\n",
            "teacher 0.179209234318\n",
            "engineer -0.0803928049452\n",
            "pilot 0.00107644989919\n",
            "computer -0.103303588739\n",
            "singer 0.185005181365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ9O2vAJi9SA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neutralize(word, g, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Removes the bias of \"word\" by projecting it on the space orthogonal to the bias axis. \n",
        "    This function ensures that gender neutral words are zero in the gender subspace.\n",
        "    \n",
        "    Arguments:\n",
        "        word -- string indicating the word to debias\n",
        "        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)\n",
        "        word_to_vec_map -- dictionary mapping words to their corresponding vectors.\n",
        "    \n",
        "    Returns:\n",
        "        e_debiased -- neutralized word vector representation of the input \"word\"\n",
        "    \"\"\"\n",
        "   \n",
        "    e = word_to_vec_map[word]\n",
        "    e_biascomponent = np.dot(e,g)/np.sum(np.square(g))*g\n",
        "    e_debiased = e-e_biascomponent\n",
        "    \n",
        "    return e_debiased"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhCH2Ombi9SH",
        "colab_type": "code",
        "outputId": "82665966-65a8-41f2-d19a-0ee34f937a8f",
        "colab": {}
      },
      "source": [
        "e = \"receptionist\"\n",
        "print(\"cosine similarity between \" + e + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[\"receptionist\"], g))\n",
        "\n",
        "e_debiased = neutralize(\"receptionist\", g, word_to_vec_map)\n",
        "print(\"cosine similarity between \" + e + \" and g, after neutralizing: \", cosine_similarity(e_debiased, g))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cosine similarity between receptionist and g, before neutralizing:  0.330779417506\n",
            "cosine similarity between receptionist and g, after neutralizing:  -5.60374039375e-17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujVjk31-i9SR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def equalize(pair, bias_axis, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Debias gender specific words by following the equalize method described in the figure above.\n",
        "    \n",
        "    Arguments:\n",
        "    pair -- pair of strings of gender specific words to debias, e.g. (\"actress\", \"actor\") \n",
        "    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender\n",
        "    word_to_vec_map -- dictionary mapping words to their corresponding vectors\n",
        "    \n",
        "    Returns\n",
        "    e_1 -- word vector corresponding to the first word\n",
        "    e_2 -- word vector corresponding to the second word\n",
        "    \"\"\"\n",
        "    \n",
        "    w1, w2 = None\n",
        "    e_w1, e_w2 = None\n",
        "    \n",
        "    mu = None\n",
        "    mu_B = None\n",
        "    mu_orth = None\n",
        "    e_w1B = None\n",
        "    e_w2B = None\n",
        "        \n",
        "    corrected_e_w1B = None\n",
        "    corrected_e_w2B = None\n",
        "\n",
        "    e1 = None\n",
        "    e2 = None\n",
        "                                                                \n",
        "    \n",
        "    return e1, e2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jCYL4kb5i9Sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"cosine similarities before equalizing:\")\n",
        "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"], g))\n",
        "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"], g))\n",
        "print()\n",
        "e1, e2 = equalize((\"man\", \"woman\"), g, word_to_vec_map)\n",
        "print(\"cosine similarities after equalizing:\")\n",
        "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, g))\n",
        "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, g))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}